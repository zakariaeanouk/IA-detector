{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and Transforming dataset"
      ],
      "metadata": {
        "id": "ZuXbYfESWEpA"
      },
      "id": "ZuXbYfESWEpA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "first we download datasets library"
      ],
      "metadata": {
        "id": "6Lqtjvbx9dAF"
      },
      "id": "6Lqtjvbx9dAF"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zPGh6LMg9aC7",
        "outputId": "fb561453-06f6-4375-c17a-d3febe31bc40"
      },
      "id": "zPGh6LMg9aC7",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.19 (from datasets)\n",
            "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, propcache, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 multiprocess-0.70.18 propcache-0.4.1 xxhash-3.6.0 yarl-1.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading HC3 dataset (Hugging face) and M4 dataset (Github)"
      ],
      "metadata": {
        "id": "Kdbq_d5E86Ex"
      },
      "id": "Kdbq_d5E86Ex"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HC3 dataset"
      ],
      "metadata": {
        "id": "KFgcR4OE9vL-"
      },
      "id": "KFgcR4OE9vL-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Downloading dataset\n",
        "- saving it as a csv file"
      ],
      "metadata": {
        "id": "Up1zIeObdO8u"
      },
      "id": "Up1zIeObdO8u"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "from io import BytesIO\n",
        "\n",
        "# ============================================================\n",
        "#   HC3 - T√©l√©charger le vrai split 'all/train' depuis HuggingFace\n",
        "# ============================================================\n",
        "\n",
        "# Original API URL to get parquet file metadata/URLs\n",
        "api_url = \"https://huggingface.co/api/datasets/Hello-SimpleAI/HC3/parquet/all/train\"\n",
        "\n",
        "print(\"üîç R√©cup√©ration de la liste des fichiers parquet...\")\n",
        "response_json = requests.get(api_url).json()\n",
        "\n",
        "parquet_urls = []\n",
        "# Robustly extract parquet URLs, accounting for potential variations in API response\n",
        "if isinstance(response_json, list):\n",
        "    if all(isinstance(item, dict) and 'url' in item for item in response_json):\n",
        "        # Expected format: list of dicts, each with a 'url' key\n",
        "        parquet_urls = [item[\"url\"] for item in response_json]\n",
        "    elif all(isinstance(item, str) for item in response_json):\n",
        "        # Fallback: if it's a list of strings (raw URLs), use them directly\n",
        "        print(\"‚ö†Ô∏è Warning: API returned a list of strings instead of dictionaries. Proceeding assuming these are direct URLs.\")\n",
        "        parquet_urls = response_json\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected API response format: List contains mixed types or missing 'url' key. Response: {response_json}\")\n",
        "elif isinstance(response_json, dict) and 'url' in response_json:\n",
        "    # If the response is a single dictionary with 'url'\n",
        "    parquet_urls = [response_json['url']]\n",
        "else:\n",
        "    raise ValueError(f\"API response is not a list or a dictionary with 'url'. Response: {response_json}\")\n",
        "\n",
        "\n",
        "print(f\"üì¶ {len(parquet_urls)} fichiers trouv√©s.\")\n",
        "\n",
        "# Dossier de sortie\n",
        "os.makedirs(\"HC3\", exist_ok=True)\n",
        "\n",
        "dfs = []\n",
        "\n",
        "# T√©l√©charger et charger chaque parquet\n",
        "for url in parquet_urls:\n",
        "    print(f\"‚¨áÔ∏è T√©l√©chargement : {url}\")\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    # Lire le contenu parquet directement en m√©moire\n",
        "    table = pq.read_table(BytesIO(r.content))\n",
        "    dfs.append(table.to_pandas())\n",
        "\n",
        "print(\"üîó Fusion de tous les blocs parquet...\")\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Export final\n",
        "csv_path = \"/content/HC3/hc3_dataset.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Export termin√© !\")\n",
        "print(\"üìÅ Fichier :\", csv_path)\n",
        "print(\"üìä Nombre total de lignes :\", len(df))\n",
        "print(\"\\nAper√ßu :\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "rLe-Qyld9ZIl",
        "outputId": "52a26497-dc10-47c5-a74a-536419fa58de"
      },
      "id": "rLe-Qyld9ZIl",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç R√©cup√©ration de la liste des fichiers parquet...\n",
            "‚ö†Ô∏è Warning: API returned a list of strings instead of dictionaries. Proceeding assuming these are direct URLs.\n",
            "üì¶ 1 fichiers trouv√©s.\n",
            "‚¨áÔ∏è T√©l√©chargement : https://huggingface.co/api/datasets/Hello-SimpleAI/HC3/parquet/all/train/0.parquet\n",
            "üîó Fusion de tous les blocs parquet...\n",
            "\n",
            "‚úÖ Export termin√© !\n",
            "üìÅ Fichier : /content/HC3/hc3_dataset.csv\n",
            "üìä Nombre total de lignes : 24322\n",
            "\n",
            "Aper√ßu :\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  id                                           question  \\\n",
              "0  0  Why is every book I hear about a \" NY Times # ...   \n",
              "1  1  If salt is so bad for cars , why do we use it ...   \n",
              "2  2  Why do we still have SD TV channels when HD lo...   \n",
              "3  3  Why has nobody assassinated Kim Jong - un He i...   \n",
              "4  4  How was airplane technology able to advance so...   \n",
              "\n",
              "                                       human_answers  \\\n",
              "0  [Basically there are many categories of \" Best...   \n",
              "1  [salt is good for not dying in car crashes and...   \n",
              "2  [The way it works is that old TV stations got ...   \n",
              "3  [You ca n't just go around assassinating the l...   \n",
              "4  [Wanting to kill the shit out of Germans drive...   \n",
              "\n",
              "                                     chatgpt_answers       source  \n",
              "0  [There are many different best seller lists th...  reddit_eli5  \n",
              "1  [Salt is used on roads to help melt ice and sn...  reddit_eli5  \n",
              "2  [There are a few reasons why we still have SD ...  reddit_eli5  \n",
              "3  [It is generally not acceptable or ethical to ...  reddit_eli5  \n",
              "4  [After the Wright Brothers made the first powe...  reddit_eli5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00dcc399-a655-40a7-a079-dd95e98aa1fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>human_answers</th>\n",
              "      <th>chatgpt_answers</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Why is every book I hear about a \" NY Times # ...</td>\n",
              "      <td>[Basically there are many categories of \" Best...</td>\n",
              "      <td>[There are many different best seller lists th...</td>\n",
              "      <td>reddit_eli5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>If salt is so bad for cars , why do we use it ...</td>\n",
              "      <td>[salt is good for not dying in car crashes and...</td>\n",
              "      <td>[Salt is used on roads to help melt ice and sn...</td>\n",
              "      <td>reddit_eli5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why do we still have SD TV channels when HD lo...</td>\n",
              "      <td>[The way it works is that old TV stations got ...</td>\n",
              "      <td>[There are a few reasons why we still have SD ...</td>\n",
              "      <td>reddit_eli5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Why has nobody assassinated Kim Jong - un He i...</td>\n",
              "      <td>[You ca n't just go around assassinating the l...</td>\n",
              "      <td>[It is generally not acceptable or ethical to ...</td>\n",
              "      <td>reddit_eli5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>How was airplane technology able to advance so...</td>\n",
              "      <td>[Wanting to kill the shit out of Germans drive...</td>\n",
              "      <td>[After the Wright Brothers made the first powe...</td>\n",
              "      <td>reddit_eli5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00dcc399-a655-40a7-a079-dd95e98aa1fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-00dcc399-a655-40a7-a079-dd95e98aa1fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-00dcc399-a655-40a7-a079-dd95e98aa1fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a7c9f159-68ed-49a4-b1b6-7cd773eccf6d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7c9f159-68ed-49a4-b1b6-7cd773eccf6d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a7c9f159-68ed-49a4-b1b6-7cd773eccf6d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 24322,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24322,\n        \"samples\": [\n          \"1227\",\n          \"2842\",\n          \"14563\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 23492,\n        \"samples\": [\n          \"On a donation site ( ie : kickstarter ) what keeps people from taking the donation money and running ? I make a great idea , get funded millions of dollars , and run with it . what stops me ? Please explain like I'm five.\",\n          \"How did old ships push off the dock ? Like back in the 1700 's , how did large ships leave port ? Mostly how did they push off the dock if winds were blowing inland ? Explain like I'm five.\",\n          \"How was western Europe so unprepared for Hitler 's invasion and the start of WW2 ? Germany lost WW1 and by the time WW2 roles around Germany is able to sweep across most of Europe with little resistance . How was the continent caught so unprepared ? Explain like I'm five. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"human_answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chatgpt_answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"open_qa\",\n          \"medicine\",\n          \"wiki_csai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### M4 dataset"
      ],
      "metadata": {
        "id": "_IVDsmKTY5so"
      },
      "id": "_IVDsmKTY5so"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- downloading dataset (.jsonl) from github\n",
        "- transforming .jsonl to csv"
      ],
      "metadata": {
        "id": "X4IwjFOrY-mM"
      },
      "id": "X4IwjFOrY-mM"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "sources = [\"arxiv\", \"reddit\", \"peerread\", \"wikihow\"]\n",
        "\n",
        "# Define the actual existing models for each source based on the GitHub repository\n",
        "source_models = {\n",
        "    \"arxiv\": [\"davinci\", \"chatGPT\", \"cohere\", \"flant5\", \"dolly\"],\n",
        "    \"reddit\": [\"davinci\", \"chatGPT\", \"cohere\", \"flant5\", \"dolly\"],\n",
        "    \"peerread\": [\"davinci\", \"cohere\", \"dolly\"],\n",
        "    \"wikihow\": [\"davinci\", \"chatGPT\", \"cohere\"],\n",
        "}\n",
        "\n",
        "for src in sources:\n",
        "  # Iterate only over models known to exist for the current source\n",
        "  if src in source_models:\n",
        "    for model in source_models[src]:\n",
        "\n",
        "      url = f\"https://raw.githubusercontent.com/mbzuai-nlp/M4/main/data/{src}_{model}.jsonl\"\n",
        "\n",
        "      # Create the M4 directory if it doesn't exist\n",
        "      if not os.path.exists(\"M4\"):\n",
        "        os.makedirs(\"M4\")\n",
        "\n",
        "      response = requests.get(url)\n",
        "\n",
        "      if response.status_code == 200:\n",
        "\n",
        "          with open(f\"./M4/{src}_{model}.jsonl\", \"wb\") as file:\n",
        "              file.write(response.content)\n",
        "          print(f\"Successfully downloaded: {src}_{model}.jsonl\")\n",
        "      else:\n",
        "\n",
        "          print(f\"Error status code {response.status_code}\")\n",
        "          print(f\"url : {url}\")\n",
        "  else:\n",
        "    print(f\"No models defined for source: {src}. Skipping.\")"
      ],
      "metadata": {
        "id": "W9rH_9_ZZNA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050b59ae-bd80-411c-cd50-0963ebd3cc24"
      },
      "id": "W9rH_9_ZZNA7",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded: arxiv_davinci.jsonl\n",
            "Successfully downloaded: arxiv_chatGPT.jsonl\n",
            "Successfully downloaded: arxiv_cohere.jsonl\n",
            "Successfully downloaded: arxiv_flant5.jsonl\n",
            "Successfully downloaded: arxiv_dolly.jsonl\n",
            "Successfully downloaded: reddit_davinci.jsonl\n",
            "Successfully downloaded: reddit_chatGPT.jsonl\n",
            "Successfully downloaded: reddit_cohere.jsonl\n",
            "Successfully downloaded: reddit_flant5.jsonl\n",
            "Successfully downloaded: reddit_dolly.jsonl\n",
            "Successfully downloaded: peerread_davinci.jsonl\n",
            "Successfully downloaded: peerread_cohere.jsonl\n",
            "Successfully downloaded: peerread_dolly.jsonl\n",
            "Successfully downloaded: wikihow_davinci.jsonl\n",
            "Successfully downloaded: wikihow_chatGPT.jsonl\n",
            "Successfully downloaded: wikihow_cohere.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "directory = '/content/M4'\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "  filename = os.path.join(directory, filename)\n",
        "\n",
        "  if os.path.isfile(filename):\n",
        "\n",
        "    data = []\n",
        "    invalide_file = False\n",
        "\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                # parsing line json and adding it to data\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(\"skipping invalid json line file\")\n",
        "                print(filename,\"\\n\")\n",
        "                invalide_file = True\n",
        "                break\n",
        "\n",
        "    if not invalide_file:\n",
        "\n",
        "      dataset = pd.DataFrame(data)\n",
        "\n",
        "      # specifying the output dir and file name\n",
        "      csv_filename = os.path.splitext(filename)[0] + '.csv'\n",
        "\n",
        "      # saving csv\n",
        "      dataset.to_csv(csv_filename, index=False)\n",
        "\n",
        "      # removing the .jsonl file\n",
        "      os.remove(filename)\n",
        "\n"
      ],
      "metadata": {
        "id": "PoEsTzhEZO9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1b9bde-e9e9-428f-bab3-0409da53302f"
      },
      "id": "PoEsTzhEZO9I",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping invalid json line file\n",
            "/content/M4/arxiv_dolly.jsonl \n",
            "\n",
            "skipping invalid json line file\n",
            "/content/M4/reddit_dolly.jsonl \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Human written text dataset"
      ],
      "metadata": {
        "id": "GrgKH-mWWQbC"
      },
      "id": "GrgKH-mWWQbC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human written from GPT2-outputs"
      ],
      "metadata": {
        "id": "qoHMb58ANRn2"
      },
      "id": "qoHMb58ANRn2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- downloading dataset\n",
        "- saving it as a csv file"
      ],
      "metadata": {
        "id": "pyKcBv3XXAcZ"
      },
      "id": "pyKcBv3XXAcZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "subdir = 'data'\n",
        "if not os.path.exists(subdir):\n",
        "    os.makedirs(subdir)\n",
        "subdir = subdir.replace('\\\\','/')\n",
        "\n",
        "for ds in [\n",
        "    'webtext',\n",
        "\n",
        "    # '''\n",
        "    #   other files are for the LLM generated dataset who have been training (fine-tuned) on webtext dataset.\n",
        "    #   no need to download them because we need LLM generated text that have not been fine tuned\n",
        "    # '''\n",
        "\n",
        "    # 'small-117M',  'small-117M-k40',\n",
        "    # 'medium-345M', 'medium-345M-k40',\n",
        "    # 'large-762M',  'large-762M-k40',\n",
        "    # 'xl-1542M',    'xl-1542M-k40',\n",
        "]:\n",
        "    for split in ['train']:\n",
        "        filename = ds + \".\" + split + '.jsonl'\n",
        "        r = requests.get(\"https://openaipublic.azureedge.net/gpt-2/output-dataset/v1/\" + filename, stream=True)\n",
        "\n",
        "        with open(os.path.join(subdir, filename), 'wb') as f:\n",
        "            file_size = int(r.headers[\"content-length\"])\n",
        "            chunk_size = 1000\n",
        "            with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
        "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(chunk_size)"
      ],
      "metadata": {
        "id": "ww9PpMDKV0EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3674d66d-a7df-4857-d20c-88feb2a91bd0"
      },
      "id": "ww9PpMDKV0EM",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching webtext.train.jsonl: 679Mit [01:28, 7.68Mit/s]                                             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming the .jsonl dataset to pandas Dataframe\n",
        "\n",
        "- selecting texts that have more than 50 words and less than 500 words\n",
        "- selecting 20000 exemples\n",
        "- finally transforming the dataframe to csv then downloading it"
      ],
      "metadata": {
        "id": "Ar0m7yq1XEbe"
      },
      "id": "Ar0m7yq1XEbe"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# transforming jsonl to dataframe\n",
        "data = []\n",
        "with open('/content/data/webtext.train.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            # parsing line json and adding it to data\n",
        "            data.append(json.loads(line))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(\"skipping invalid json line\")\n",
        "\n",
        "# dir to stock human text dataset\n",
        "os.makedirs(\"Human-text-dataset\", exist_ok=True)\n",
        "\n",
        "dataset = pd.DataFrame(data)\n",
        "\n",
        "# selecting just columns that have not been truncated\n",
        "dataset = dataset[dataset['ended'] == True]\n",
        "\n",
        "\n",
        "# selecting texts that have  50 < words < 500\n",
        "sub_dataset = dataset[dataset[\"length\"] < 500 ]\n",
        "\n",
        "sub_dataset = sub_dataset[sub_dataset[\"length\"] > 50 ]\n",
        "\n",
        "# shape of the subdataset\n",
        "print(sub_dataset.shape)\n",
        "\n",
        "# selecting 35000 text from the dataset randomly\n",
        "sub_dataset = sub_dataset.sample(n=25000, random_state=42)\n",
        "\n",
        "# adding label\n",
        "sub_dataset[\"label\"] = \"human\"\n",
        "\n",
        "# droping unecessary columns\n",
        "drop_columns = [col for col in sub_dataset.columns if col not in {'text','label'}]\n",
        "sub_dataset.drop(columns=drop_columns, inplace=True)\n",
        "\n",
        "\n",
        "print(sub_dataset.shape)\n",
        "\n",
        "# transforming the dataset to csv file\n",
        "sub_dataset.to_csv(\"Human-text-dataset/GPT2-outputs-human-written-dataset.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0zI3QB4XREc",
        "outputId": "49509c11-4fe1-4c5e-c141-fd911d93e065"
      },
      "id": "V0zI3QB4XREc",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(113557, 4)\n",
            "(25000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human written from HC3"
      ],
      "metadata": {
        "id": "RVSNzGkUNK5I"
      },
      "id": "RVSNzGkUNK5I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " - changing dataset structure to match other datasets\n",
        " - deleting unecessary columns\n",
        " - Downloading some of the humain answers from the HC3 dataset"
      ],
      "metadata": {
        "id": "ImRYGb_62_tC"
      },
      "id": "ImRYGb_62_tC"
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/HC3/hc3_dataset.csv')\n",
        "\n",
        "'''\n",
        "    dataset will have a structure like this :\n",
        "   text, label\n",
        "   text : the text itself\n",
        "   label : the label of the text (human or ai)\n",
        "\n",
        "'''\n",
        "\n",
        "# deleting the [ ] from human_answers\n",
        "data['human_answers'] = data['human_answers'].str.replace(r'[\\[\\]]', '', regex=True)\n",
        "\n",
        "\n",
        "# getting the length of the human_answers\n",
        "data[\"length\"] = data['human_answers'].apply(lambda x: len(x.split()))\n",
        "\n",
        "\n",
        "# selecting texts that have  50 < words < 500\n",
        "sub_dataset = data[data[\"length\"] < 500 ]\n",
        "\n",
        "sub_dataset = sub_dataset[sub_dataset[\"length\"] > 50 ]\n",
        "\n",
        "\n",
        "data = sub_dataset\n",
        "\n",
        "# droping unecessary columns\n",
        "data.drop(columns = [\"source\",\"chatgpt_answers\",\"question\",\"id\",\"length\"], inplace=True)\n",
        "\n",
        "\n",
        "# adding label 'human'\n",
        "data[\"label\"] = \"human\"\n",
        "\n",
        "# renaming columns\n",
        "data.rename(columns = { 'human_answers' : 'text'}, inplace=True)\n",
        "\n",
        "\n",
        "print(\"shape of dataset: \",data.shape)\n",
        "\n",
        "# exporting dataset as a csv file\n",
        "data.to_csv(\"Human-text-dataset/HC3-Human-written-dataset.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcP-fqhL1GI-",
        "outputId": "8dc67369-02c0-4aab-9d11-9fc5d2cc26f6"
      },
      "id": "CcP-fqhL1GI-",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of dataset:  (17784, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human written from M4"
      ],
      "metadata": {
        "id": "OHSCCxV6Zpej"
      },
      "id": "OHSCCxV6Zpej"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " - changing dataset structure to match other datasets\n",
        " - deleting unecessary columns\n",
        " - Downloading some of the humain answers from the M4 dataset"
      ],
      "metadata": {
        "id": "SOb8hfckZtMW"
      },
      "id": "SOb8hfckZtMW"
    },
    {
      "cell_type": "code",
      "source": [
        "# human written text\n",
        "'''\n",
        "        dataset will have a structure like this :\n",
        "      , text, label\n",
        "      text : the text itself\n",
        "      label : the label of the text (human or ai)\n",
        "\n",
        "'''\n",
        "\n",
        "directory = '/content/M4'\n",
        "output_directory = '/content/Human-text-dataset'\n",
        "\n",
        "\n",
        "#create output dir\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# create an empty dataframe to store all the values\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "required_columns = {'human_text'}\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "  if filename.endswith('.csv'): # processing only csv files\n",
        "\n",
        "    filepath = os.path.join(directory, filename)\n",
        "\n",
        "    print(f\"Processing file: {filename}\")\n",
        "\n",
        "    data = pd.read_csv(filepath)\n",
        "\n",
        "    if not required_columns.issubset(data.columns):\n",
        "      print(f\"The file {filename} does not contain the required column 'human_text'. Skipping.\")\n",
        "      continue\n",
        "\n",
        "    # getting the length of the human_text\n",
        "    try:\n",
        "      data[\"length\"] = data['human_text'].apply(lambda x: len(x.split()))\n",
        "    except Exception as e:\n",
        "\n",
        "      print(f\"the file {filename} was not processed\")\n",
        "      continue\n",
        "\n",
        "    # selecting texts that have  50 < words < 500\n",
        "    sub_dataset = data[data[\"length\"] < 500 ]\n",
        "\n",
        "    sub_dataset = sub_dataset[sub_dataset[\"length\"] > 50 ]\n",
        "\n",
        "\n",
        "    # droping unecessary columns\n",
        "    drop_columns = [col for col in data.columns if col not in {'human_text'}]\n",
        "    sub_dataset.drop(columns=drop_columns, inplace=True)\n",
        "\n",
        "    # assign new dataset to data variable\n",
        "    data = sub_dataset\n",
        "\n",
        "    # adding label 'human'\n",
        "    data[\"label\"] = \"human\"\n",
        "\n",
        "    # renaming columns\n",
        "    data.rename(columns = { 'human_text' : 'text'}, inplace=True)\n",
        "\n",
        "    # concatinating data to forme a global dataset\n",
        "    all_data = pd.concat([all_data, data], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "output_filepath = os.path.join(output_directory, \"M4-Human-written-dataset.csv\")\n",
        "\n",
        "print(\"shape of dataset: \",all_data.shape)\n",
        "\n",
        "all_data.to_csv(output_filepath, index=False)\n",
        "\n",
        "\n",
        "count = all_data.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY-l7tg8Zw86",
        "outputId": "3449497f-7eac-498f-cd2f-d0466781cd59"
      },
      "id": "yY-l7tg8Zw86",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: peerread_cohere.csv\n",
            "Processing file: peerread_chatgpt.csv\n",
            "Processing file: arxiv_davinci.csv\n",
            "Processing file: arxiv_chatGPT.csv\n",
            "Processing file: reddit_davinci.csv\n",
            "Processing file: wikihow_davinci.csv\n",
            "Processing file: peerread_davinci.csv\n",
            "Processing file: wikihow_cohere.csv\n",
            "the file wikihow_cohere.csv was not processed\n",
            "Processing file: reddit_cohere.csv\n",
            "Processing file: arxiv_cohere.csv\n",
            "Processing file: reddit_flant5.csv\n",
            "Processing file: arxiv_flant5.csv\n",
            "Processing file: wikihow_chatGPT.csv\n",
            "Processing file: peerread_dolly.csv\n",
            "Processing file: reddit_chatGPT.csv\n",
            "shape of dataset:  (22796, 2)\n",
            "Number of duplicate rows: 16521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second LLM generated dataset"
      ],
      "metadata": {
        "id": "2veI6ehuX5Gb"
      },
      "id": "2veI6ehuX5Gb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ai generated text from HC3"
      ],
      "metadata": {
        "id": "ln99IM3-zSHT"
      },
      "id": "ln99IM3-zSHT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- changing dataset structure to match other datasets\n",
        "- deleting unecessary columns"
      ],
      "metadata": {
        "id": "zIdVdMlCYbRL"
      },
      "id": "zIdVdMlCYbRL"
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/HC3/hc3_dataset.csv')\n",
        "\n",
        "# '''\n",
        "#     dataset will have a structure like this :\n",
        "#    , text, label\n",
        "#    text : the text itself\n",
        "#    label : the label of the text (human or ai)\n",
        "\n",
        "# '''\n",
        "\n",
        "# deleting the [ ] from chatgpt_answers\n",
        "data['chatgpt_answers'] = data['chatgpt_answers'].str.replace(r'[\\[\\]]', '', regex=True)\n",
        "\n",
        "\n",
        "# getting the length of the chatgpt_answers\n",
        "data[\"length\"] = data['chatgpt_answers'].apply(lambda x: len(x.split()))\n",
        "\n",
        "\n",
        "# selecting texts that have  50 < words < 500\n",
        "sub_dataset = data[data[\"length\"] < 500 ]\n",
        "\n",
        "sub_dataset = sub_dataset[sub_dataset[\"length\"] > 50 ]\n",
        "\n",
        "\n",
        "data = sub_dataset\n",
        "\n",
        "# droping unecessary columns\n",
        "data.drop(columns = [\"source\",\"human_answers\",\"question\",\"id\",\"length\"], inplace=True)\n",
        "\n",
        "\n",
        "# adding label 'Ai'\n",
        "data[\"label\"] = \"Ai\"\n",
        "\n",
        "# renaming columns\n",
        "data.rename(columns = { 'chatgpt_answers' : 'text'}, inplace=True)\n",
        "\n",
        "\n",
        "print(\"shape of dataset: \",data.shape)\n",
        "\n",
        "data.head()\n",
        "\n",
        "\n",
        "os.makedirs(\"Ai-generated-text-dataset\", exist_ok=True)\n",
        "\n",
        "# exporting dataset as a csv file\n",
        "sub_dataset.to_csv(\"Ai-generated-text-dataset/HC3-Ai-generated-dataset.csv\")\n",
        "\n",
        "data.head()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "WmmOepL1T6C4",
        "outputId": "bba9b0f6-339f-4035-82ce-1e6a1a1177bd"
      },
      "id": "WmmOepL1T6C4",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of dataset:  (23342, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text label\n",
              "0  'There are many different best seller lists th...    Ai\n",
              "1  \"Salt is used on roads to help melt ice and sn...    Ai\n",
              "2  \"There are a few reasons why we still have SD ...    Ai\n",
              "3  'It is generally not acceptable or ethical to ...    Ai\n",
              "4  'After the Wright Brothers made the first powe...    Ai"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b72aa3a6-cb78-46e7-877b-177b0e2e7df7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'There are many different best seller lists th...</td>\n",
              "      <td>Ai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Salt is used on roads to help melt ice and sn...</td>\n",
              "      <td>Ai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"There are a few reasons why we still have SD ...</td>\n",
              "      <td>Ai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'It is generally not acceptable or ethical to ...</td>\n",
              "      <td>Ai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>'After the Wright Brothers made the first powe...</td>\n",
              "      <td>Ai</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b72aa3a6-cb78-46e7-877b-177b0e2e7df7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b72aa3a6-cb78-46e7-877b-177b0e2e7df7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b72aa3a6-cb78-46e7-877b-177b0e2e7df7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-98a7d2c1-b90c-4eeb-b621-610ebe6e0297\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-98a7d2c1-b90c-4eeb-b621-610ebe6e0297')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-98a7d2c1-b90c-4eeb-b621-610ebe6e0297 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 23342,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22828,\n        \"samples\": [\n          \"'El Al, like most airlines, uses a dynamic currency exchange rate that is based on the current market exchange rate for the currencies involved in the transaction. The exchange rate that El Al uses for a particular transaction may be different from the exchange rate that is quoted by banks or currency exchange services, as it is based on the rate that is available to El Al at the time the transaction is processed. It is not uncommon for exchange rates to fluctuate over time, so the exchange rate that El Al uses for a particular transaction may be different from the exchange rate that is used for other transactions, even if they are completed around the same time.'\",\n          \"\\\"There are many different people and organizations involved in the illegal drug trade, and it can be difficult to keep track of all of them. When drugs are produced in one country and then smuggled into another country, it is often the smugglers who get caught by the authorities, rather than the people who are buying the drugs. Additionally, the illegal drug trade is a very secretive and dangerous business, so it can be hard for people to find out what is really happening. The people who are moving the drugs across the border and selling them to buyers in other countries are usually trying to avoid being caught by the police, so they don't want to draw attention to themselves. This is one reason why we may hear more about drug cartels in Mexico than about the people in other countries who are buying the drugs.\\\"\",\n          \"'Sure! Vacuum tubes were some of the first electronic devices used in computers and other electronic equipment. They were made of glass and had a vacuum inside, which is why they were called vacuum tubes. The vacuum was important because it allowed electricity to flow through the tube without any interference. Vacuum tubes were used in many different ways, but they were mostly used to amplify signals or switch electronic circuits on and off.As technology advanced, people started to use smaller and more efficient electronic devices called transistors. Transistors are made of a special type of material called semiconductor, which can be used to control the flow of electricity. Transistors are much smaller than vacuum tubes and can be used to do many of the same things, but they are more reliable and use less energy.Today, most electronic devices use microchips, which are tiny computer chips that can be found in everything from phones and laptops to cars and appliances. Microchips are made up of many tiny transistors and other components that can be used to store and process information. They are even smaller than transistors and can do much more complex tasks.So to sum it up, vacuum tubes were some of the first electronic devices used in computers and other equipment, but they were eventually replaced with transistors and microchips because they were smaller, more efficient, and more reliable. Today, most electronic devices use microchips.'\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Ai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ai generated from M4"
      ],
      "metadata": {
        "id": "7zSZ-O2XZ5Yd"
      },
      "id": "7zSZ-O2XZ5Yd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- changing dataset structure to match other datasets\n",
        "- deleting unecessary columns"
      ],
      "metadata": {
        "id": "5cPYuOqmZ_r0"
      },
      "id": "5cPYuOqmZ_r0"
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import exception\n",
        "# AI written text\n",
        "\n",
        "# '''\n",
        "#         dataset will have a structure like this :\n",
        "#       , text, label\n",
        "#       text : the text itself\n",
        "#       label : the label of the text (human or ai)\n",
        "\n",
        "# '''\n",
        "\n",
        "directory = '/content/M4'\n",
        "output_directory = '/content/Ai-generated-text-dataset'\n",
        "\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "required_columns = {'machine_text'}\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "  if filename.endswith('.csv'):\n",
        "\n",
        "    filepath = os.path.join(directory, filename)\n",
        "\n",
        "    print(f\"Processing file: {filename}\")\n",
        "\n",
        "    data = pd.read_csv(filepath)\n",
        "\n",
        "    if not required_columns.issubset(data.columns):\n",
        "      print(f\"The file {filename} does not contain the required column 'human_text'. Skipping.\")\n",
        "      continue\n",
        "\n",
        "    # getting the length of the machine_text\n",
        "    try :\n",
        "      data[\"length\"] = data['machine_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "      print(f\"the file {filename} was not processed\")\n",
        "      continue\n",
        "\n",
        "\n",
        "    # selecting texts that have  50 < words < 500\n",
        "    sub_dataset = data[data[\"length\"] < 500 ]\n",
        "\n",
        "    sub_dataset = sub_dataset[sub_dataset[\"length\"] > 40 ]\n",
        "\n",
        "    # assign new dataset to data variable\n",
        "    data = sub_dataset\n",
        "\n",
        "    # droping unecessary columns\n",
        "    drop_columns = [col for col in data.columns if col not in {'machine_text'}]\n",
        "    data.drop(columns=drop_columns, inplace=True)\n",
        "\n",
        "\n",
        "    # adding label 'Ai'\n",
        "    data[\"label\"] = \"Ai\"\n",
        "\n",
        "    # renaming columns\n",
        "    data.rename(columns = { 'machine_text' : 'text'}, inplace=True)\n",
        "\n",
        "    all_data = pd.concat([all_data, data], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "output_filepath = os.path.join(output_directory, \"M4-Ai-generated-dataset.csv\")\n",
        "\n",
        "print(\"shape of dataset: \",all_data.shape)\n",
        "\n",
        "all_data.to_csv(output_filepath, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeRpU-AFZ8El",
        "outputId": "97c8a0af-edb1-4e53-a8fd-78b1f0870761"
      },
      "id": "JeRpU-AFZ8El",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: peerread_cohere.csv\n",
            "Processing file: peerread_chatgpt.csv\n",
            "Processing file: arxiv_davinci.csv\n",
            "Processing file: arxiv_chatGPT.csv\n",
            "Processing file: reddit_davinci.csv\n",
            "Processing file: wikihow_davinci.csv\n",
            "Processing file: peerread_davinci.csv\n",
            "Processing file: wikihow_cohere.csv\n",
            "Processing file: reddit_cohere.csv\n",
            "the file reddit_cohere.csv was not processed\n",
            "Processing file: arxiv_cohere.csv\n",
            "Processing file: reddit_flant5.csv\n",
            "Processing file: arxiv_flant5.csv\n",
            "Processing file: wikihow_chatGPT.csv\n",
            "Processing file: peerread_dolly.csv\n",
            "Processing file: reddit_chatGPT.csv\n",
            "shape of dataset:  (24973, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combaining both datasets"
      ],
      "metadata": {
        "id": "DF9rix9JrOxN"
      },
      "id": "DF9rix9JrOxN"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# human dataset\n",
        "\n",
        "df1 = pd.read_csv('/content/Human-text-dataset/HC3-Human-written-dataset.csv')\n",
        "df2 = pd.read_csv('/content/Human-text-dataset/M4-Human-written-dataset.csv')\n",
        "df3 = pd.read_csv('/content/Human-text-dataset/GPT2-outputs-human-written-dataset.csv')\n",
        "\n",
        "\n",
        "# droping unnamed : 0 columns for df1 and df3\n",
        "df1.drop(columns = [\"Unnamed: 0\"], inplace=True)\n",
        "df3.drop(columns = [\"Unnamed: 0\"], inplace=True)\n",
        "\n",
        "df_all_human = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "\n",
        "\n",
        "# Ai dataset\n",
        "\n",
        "df4 = pd.read_csv('/content/Ai-generated-text-dataset/HC3-Ai-generated-dataset.csv')\n",
        "df5 = pd.read_csv('/content/Ai-generated-text-dataset/M4-Ai-generated-dataset.csv')\n",
        "\n",
        "df4.drop(columns = [\"Unnamed: 0\"], inplace=True)\n",
        "\n",
        "\n",
        "df_all_Ai = pd.concat([df4, df5], ignore_index=True)\n",
        "\n",
        "# print(\"the shape of human dataset is : \", df_all_human.shape)\n",
        "print(\"the shape of Ai generated text dataest is : \", df_all_Ai.shape)\n",
        "\n",
        "\n",
        "# droping duplicates to see if there is a balance between classes\n",
        "df_all_human = df_all_human.drop_duplicates()\n",
        "df_all_Ai = df_all_Ai.drop_duplicates()\n",
        "\n",
        "print(\"The shape of human dataset after removing duplicates is:\", df_all_human.shape)\n",
        "print(\"The shape of AI generated text dataset after removing duplicates is:\", df_all_Ai.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7JHqHl6rNte",
        "outputId": "8d45a43c-2a13-4013-822b-ca61483a36a0"
      },
      "id": "H7JHqHl6rNte",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the shape of Ai generated text dataest is :  (48315, 2)\n",
            "The shape of human dataset after removing duplicates is: (47599, 2)\n",
            "The shape of AI generated text dataset after removing duplicates is: (47796, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading datasets"
      ],
      "metadata": {
        "id": "pS2qmeZBcMFk"
      },
      "id": "pS2qmeZBcMFk"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "os.makedirs(\"datasets\", exist_ok=True)\n",
        "\n",
        "df_all_human.to_csv('datasets/human_dataset.csv', index=False)\n",
        "df_all_Ai.to_csv('datasets/Ai_dataset.csv', index=False)\n",
        "\n",
        "dir = '/content/datasets'\n",
        "\n",
        "zip = '/content/datasets.zip'\n",
        "\n",
        "shutil.make_archive(zip.replace('.zip', ''), 'zip', dir)\n",
        "\n",
        "print(f\"Directories have been zipped: {zip}\")\n",
        "\n",
        "files.download(zip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xhJyJehsb5Iw",
        "outputId": "9371c341-6fdc-4912-e03f-9eec41c6b370"
      },
      "id": "xhJyJehsb5Iw",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories have been zipped: /content/datasets.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_427bdd0d-68e0-4259-8530-99da5abb8c86\", \"datasets.zip\", 40639167)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "nbreset": "https://raw.githubusercontent.com/INRIA/scikit-learn-mooc/main/notebooks/ensemble_hyperparameters.ipynb",
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}